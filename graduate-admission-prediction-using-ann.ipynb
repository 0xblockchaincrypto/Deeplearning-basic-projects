{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":228180,"sourceType":"datasetVersion","datasetId":14872}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:26:47.199266Z","iopub.execute_input":"2025-01-25T18:26:47.199591Z","iopub.status.idle":"2025-01-25T18:26:48.511573Z","shell.execute_reply.started":"2025-01-25T18:26:47.199565Z","shell.execute_reply":"2025-01-25T18:26:48.510437Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/graduate-admissions/Admission_Predict.csv\n/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport numpy as np   # For numerical operations\nimport torch         # PyTorch main package\nimport torch.nn as nn  # Neural network modules\nimport torch.optim as optim  # Optimization algorithms\nfrom torch.utils.data import DataLoader, TensorDataset  # For data handling\nfrom sklearn.model_selection import train_test_split  # For splitting data\nfrom sklearn.preprocessing import StandardScaler  # For feature scaling\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error  # Evaluation metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:29:23.334436Z","iopub.execute_input":"2025-01-25T18:29:23.334897Z","iopub.status.idle":"2025-01-25T18:29:28.146430Z","shell.execute_reply.started":"2025-01-25T18:29:23.334867Z","shell.execute_reply":"2025-01-25T18:29:28.145423Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"'''\nDATA PREPROCESSING SECTION\n\nWhy we do this:\n- Real-world data is often messy and needs cleaning\n- Neural networks work better with normalized/standardized data\n- Proper data splitting helps evaluate model performance accurately\n'''\n\n# Load dataset from CSV file\ndf = pd.read_csv('/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:30:48.700181Z","iopub.execute_input":"2025-01-25T18:30:48.700729Z","iopub.status.idle":"2025-01-25T18:30:48.722720Z","shell.execute_reply.started":"2025-01-25T18:30:48.700699Z","shell.execute_reply":"2025-01-25T18:30:48.721645Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:31:08.120783Z","iopub.execute_input":"2025-01-25T18:31:08.121137Z","iopub.status.idle":"2025-01-25T18:31:08.150594Z","shell.execute_reply.started":"2025-01-25T18:31:08.121111Z","shell.execute_reply":"2025-01-25T18:31:08.149463Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n0           1        337          118                  4  4.5   4.5  9.65   \n1           2        324          107                  4  4.0   4.5  8.87   \n2           3        316          104                  3  3.0   3.5  8.00   \n3           4        322          110                  3  3.5   2.5  8.67   \n4           5        314          103                  2  2.0   3.0  8.21   \n\n   Research  Chance of Admit   \n0         1              0.92  \n1         1              0.76  \n2         1              0.72  \n3         1              0.80  \n4         0              0.65  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Serial No.</th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:31:18.836665Z","iopub.execute_input":"2025-01-25T18:31:18.837029Z","iopub.status.idle":"2025-01-25T18:31:18.842878Z","shell.execute_reply.started":"2025-01-25T18:31:18.837001Z","shell.execute_reply":"2025-01-25T18:31:18.841973Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(500, 9)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:31:34.578229Z","iopub.execute_input":"2025-01-25T18:31:34.578581Z","iopub.status.idle":"2025-01-25T18:31:34.604966Z","shell.execute_reply.started":"2025-01-25T18:31:34.578556Z","shell.execute_reply":"2025-01-25T18:31:34.603793Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 9 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Serial No.         500 non-null    int64  \n 1   GRE Score          500 non-null    int64  \n 2   TOEFL Score        500 non-null    int64  \n 3   University Rating  500 non-null    int64  \n 4   SOP                500 non-null    float64\n 5   LOR                500 non-null    float64\n 6   CGPA               500 non-null    float64\n 7   Research           500 non-null    int64  \n 8   Chance of Admit    500 non-null    float64\ndtypes: float64(4), int64(5)\nmemory usage: 35.3 KB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:31:48.190826Z","iopub.execute_input":"2025-01-25T18:31:48.191156Z","iopub.status.idle":"2025-01-25T18:31:48.199481Z","shell.execute_reply.started":"2025-01-25T18:31:48.191132Z","shell.execute_reply":"2025-01-25T18:31:48.198467Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Clean column names (remove spaces and trailing characters)\n# This helps prevent errors when accessing columns\ndf.columns = df.columns.str.strip().str.replace(' ', '_')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:32:10.480296Z","iopub.execute_input":"2025-01-25T18:32:10.480766Z","iopub.status.idle":"2025-01-25T18:32:10.487009Z","shell.execute_reply.started":"2025-01-25T18:32:10.480728Z","shell.execute_reply":"2025-01-25T18:32:10.485546Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Drop irrelevant column (Serial Number doesn't affect admission chances)\n# Always remove non-predictive features\ndf = df.drop('Serial_No.', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:33:06.095695Z","iopub.execute_input":"2025-01-25T18:33:06.096254Z","iopub.status.idle":"2025-01-25T18:33:06.104867Z","shell.execute_reply.started":"2025-01-25T18:33:06.096215Z","shell.execute_reply":"2025-01-25T18:33:06.102833Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Separate features (input variables) and target (output variable)\n# X contains all factors affecting admission chance\n# y contains the actual admission probabilities we want to predict\nX = df.drop('Chance_of_Admit', axis=1).values  # Features matrix\ny = df['Chance_of_Admit'].values.reshape(-1, 1)  # Target variable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:33:25.015260Z","iopub.execute_input":"2025-01-25T18:33:25.015798Z","iopub.status.idle":"2025-01-25T18:33:25.023404Z","shell.execute_reply.started":"2025-01-25T18:33:25.015764Z","shell.execute_reply":"2025-01-25T18:33:25.021898Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Split data into training (80%) and test (20%) sets\n# We keep test set separate to evaluate model performance on unseen data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42  # random_state for reproducibility\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:33:35.140953Z","iopub.execute_input":"2025-01-25T18:33:35.141401Z","iopub.status.idle":"2025-01-25T18:33:35.147959Z","shell.execute_reply.started":"2025-01-25T18:33:35.141368Z","shell.execute_reply":"2025-01-25T18:33:35.146459Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Standardize features (mean=0, std=1)\n# Neural networks require scaled data for optimal performance\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)  # Fit and transform training data\nX_test = scaler.transform(X_test)        # Transform test data using training fit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:33:45.698734Z","iopub.execute_input":"2025-01-25T18:33:45.699220Z","iopub.status.idle":"2025-01-25T18:33:45.707694Z","shell.execute_reply.started":"2025-01-25T18:33:45.699185Z","shell.execute_reply":"2025-01-25T18:33:45.706381Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:42:14.155116Z","iopub.execute_input":"2025-01-25T18:42:14.155522Z","iopub.status.idle":"2025-01-25T18:42:14.162441Z","shell.execute_reply.started":"2025-01-25T18:42:14.155491Z","shell.execute_reply":"2025-01-25T18:42:14.161295Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([[ 0.38998634,  0.6024183 , -0.09829757, ...,  0.56498381,\n         0.4150183 ,  0.89543386],\n       [-0.06640493,  0.6024183 ,  0.7754586 , ...,  1.65149114,\n        -0.06785154, -1.11677706],\n       [-1.25302222, -0.87691722, -0.09829757, ..., -0.52152352,\n        -0.13445427, -1.11677706],\n       ...,\n       [-1.34430047, -1.37002906, -1.8458099 , ..., -1.60803084,\n        -2.2157898 , -1.11677706],\n       [-0.7053527 , -0.38380538, -0.97205374, ...,  0.56498381,\n        -1.49981038, -1.11677706],\n       [-0.24896144, -0.21943477, -0.97205374, ...,  0.02173015,\n        -0.55072138, -1.11677706]])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Convert numpy arrays to PyTorch tensors\n# PyTorch works with tensors instead of regular arrays\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:33:56.696415Z","iopub.execute_input":"2025-01-25T18:33:56.696837Z","iopub.status.idle":"2025-01-25T18:33:56.732948Z","shell.execute_reply.started":"2025-01-25T18:33:56.696810Z","shell.execute_reply":"2025-01-25T18:33:56.731835Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create DataLoader for batch training\n# Batches help with:\n# 1. Memory efficiency (don't load all data at once)\n# 2. Better gradient updates\n# 3. Random shuffling reduces overfitting\nbatch_size = 32  # Number of samples per batch (common to use powers of 2)\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:34:08.915837Z","iopub.execute_input":"2025-01-25T18:34:08.916303Z","iopub.status.idle":"2025-01-25T18:34:08.925904Z","shell.execute_reply.started":"2025-01-25T18:34:08.916267Z","shell.execute_reply":"2025-01-25T18:34:08.924496Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"'''\nNEURAL NETWORK ARCHITECTURE\n\nWhy this structure:\n- Fully connected layers (Linear layers) are good for tabular data\n- ReLU activation adds non-linearity (neural networks need this to learn complex patterns)\n- Dropout prevents overfitting (randomly turns off neurons during training)\n- Three layers: Input -> Hidden -> Hidden -> Output (common starting point)\n'''","metadata":{}},{"cell_type":"code","source":"class AdmissionPredictor(nn.Module):\n    def __init__(self, input_dim):\n        super(AdmissionPredictor, self).__init__()\n        # Layer 1: Input layer to first hidden layer\n        self.fc1 = nn.Linear(input_dim, 64)  # 64 neurons in first hidden layer\n        # Layer 2: Hidden layer to another hidden layer\n        self.fc2 = nn.Linear(64, 32)         # 32 neurons in second hidden layer\n        # Layer 3: Final hidden layer to output\n        self.fc3 = nn.Linear(32, 1)          # 1 output neuron (regression task)\n        \n        # Activation function (introduces non-linearity)\n        self.relu = nn.ReLU()  # Simple and effective activation function\n        \n        # Regularization (prevents model from memorizing training data)\n        self.dropout = nn.Dropout(0.2)  # Randomly disable 20% of neurons during training\n        \n    def forward(self, x):\n        # Forward pass through network\n        x = self.relu(self.fc1(x))  # Activation after first layer\n        x = self.dropout(x)         # Apply dropout\n        x = self.relu(self.fc2(x))  # Activation after second layer\n        x = self.dropout(x)         # Apply dropout\n        x = self.fc3(x)             # Final output (no activation for regression)\n        return x\n\n# Initialize model with proper input dimension\ninput_dim = X_train.shape[1]  # Number of features (7 in this dataset)\nmodel = AdmissionPredictor(input_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:34:52.852389Z","iopub.execute_input":"2025-01-25T18:34:52.852715Z","iopub.status.idle":"2025-01-25T18:34:52.873737Z","shell.execute_reply.started":"2025-01-25T18:34:52.852694Z","shell.execute_reply":"2025-01-25T18:34:52.872538Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"'''\nLOSS FUNCTION AND OPTIMIZER\n\nWhy MSE and Adam:\n- MSE (Mean Squared Error) is common for regression problems\n- Adam optimizer combines benefits of two other optimizers (AdaGrad and RMSProp)\n- Learning rate (0.001) is a good starting point for many problems\n'''","metadata":{}},{"cell_type":"code","source":"criterion = nn.MSELoss()  # Measures average squared difference\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adaptive learning rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:35:24.539990Z","iopub.execute_input":"2025-01-25T18:35:24.540388Z","iopub.status.idle":"2025-01-25T18:35:26.970838Z","shell.execute_reply.started":"2025-01-25T18:35:24.540359Z","shell.execute_reply":"2025-01-25T18:35:26.969815Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"\nTRAINING LOOP\n\nKey concepts:\n- Epochs: Complete passes through the training data\n- Batch processing: Updates weights after each batch (not whole dataset)\n- Backpropagation: Calculates gradients of loss w.r.t. parameters\n- Gradient descent: Updates weights to minimize loss\n","metadata":{}},{"cell_type":"code","source":"num_epochs = 200  # Number of complete passes through dataset\nfor epoch in range(num_epochs):\n    for inputs, targets in train_loader:\n        # Forward pass: Compute predictions\n        outputs = model(inputs)\n        \n        # Calculate loss (difference between predictions and true values)\n        loss = criterion(outputs, targets)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()  # Clear previous gradients\n        loss.backward()        # Compute gradients through backpropagation\n        optimizer.step()       # Update weights using gradients\n        \n    # Print progress every 20 epochs\n    if (epoch+1) % 20 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:35:56.309207Z","iopub.execute_input":"2025-01-25T18:35:56.309814Z","iopub.status.idle":"2025-01-25T18:36:01.353873Z","shell.execute_reply.started":"2025-01-25T18:35:56.309782Z","shell.execute_reply":"2025-01-25T18:36:01.352696Z"}},"outputs":[{"name":"stdout","text":"Epoch [20/200], Loss: 0.0289\nEpoch [40/200], Loss: 0.0142\nEpoch [60/200], Loss: 0.0081\nEpoch [80/200], Loss: 0.0072\nEpoch [100/200], Loss: 0.0059\nEpoch [120/200], Loss: 0.0058\nEpoch [140/200], Loss: 0.0056\nEpoch [160/200], Loss: 0.0038\nEpoch [180/200], Loss: 0.0054\nEpoch [200/200], Loss: 0.0060\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"'''\nEVALUATION\n\nWhy we use both MAE and MSE:\n- MAE (Mean Absolute Error) is easier to interpret (average error)\n- MSE (Mean Squared Error) penalizes larger errors more heavily\n- Both help understand model performance in different ways\n'''","metadata":{}},{"cell_type":"code","source":"model.eval()  # Set model to evaluation mode (disables dropout)\nwith torch.no_grad():  # Disable gradient calculation for efficiency\n    test_outputs = model(X_test_tensor)\n    test_loss = criterion(test_outputs, y_test_tensor)\n    mae = mean_absolute_error(y_test, test_outputs.numpy())\n    mse = mean_squared_error(y_test, test_outputs.numpy())\n    \nprint(f'\\nTest MSE: {test_loss.item():.4f}')\nprint(f'Test MAE: {mae:.4f}')\nprint(f'Test MSE: {mse:.4f}')\n\n# Example prediction to show actual vs predicted\nexample_data = X_test_tensor[0:1]  # First test sample\nprediction = model(example_data)\nprint(f'\\nExample prediction:')\nprint(f'Actual: {y_test[0][0]:.4f}')\nprint(f'Predicted: {prediction.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:36:25.940995Z","iopub.execute_input":"2025-01-25T18:36:25.941369Z","iopub.status.idle":"2025-01-25T18:36:25.953162Z","shell.execute_reply.started":"2025-01-25T18:36:25.941338Z","shell.execute_reply":"2025-01-25T18:36:25.952106Z"}},"outputs":[{"name":"stdout","text":"\nTest MSE: 0.0040\nTest MAE: 0.0453\nTest MSE: 0.0040\n\nExample prediction:\nActual: 0.9300\nPredicted: 0.9032\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}