{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SvBKztyCAfT"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.optim as optim  # Optimization algorithms\n",
        "from torchvision import datasets, transforms  # MNIST dataset and transformations\n",
        "from torch.utils.data import DataLoader  # For batch loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration (Use GPU if available, otherwise CPU)\n",
        "# Why? GPUs are faster for matrix operations in neural networks\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3_2hmxx9CLHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (Configuration settings for our model)\n",
        "input_size = 784    # 28x28 pixels flattened to 1D array (784 values)\n",
        "hidden_size1 = 128  # Number of neurons in first hidden layer\n",
        "hidden_size2 = 64   # Number of neurons in second hidden layer\n",
        "num_classes = 10    # Output classes (digits 0-9)\n",
        "num_epochs = 10     # Number of times we'll go through the entire dataset\n",
        "batch_size = 64     # Number of samples processed before updating weights\n",
        "learning_rate = 0.001  # Step size for parameter updates"
      ],
      "metadata": {
        "id": "M_pbS5wxCW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset transformations\n",
        "# Why normalize? Makes training more stable by keeping values in small range\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST-specific mean and std\n",
        "])"
      ],
      "metadata": {
        "id": "XsSOsCg_CbM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "# Why separate train/test sets? To evaluate on unseen data (avoid cheating)\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")"
      ],
      "metadata": {
        "id": "HFB6cKUKChjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "a9YYRdvlCkZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loaders (handles batch loading and shuffling)\n",
        "# Why batches? More efficient than single-sample processing\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True  # Why shuffle? Prevent order bias in learning\n",
        ")"
      ],
      "metadata": {
        "id": "YAdLJgMDCrVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False  # No need to shuffle test data\n",
        ")"
      ],
      "metadata": {
        "id": "tIrk2MNlCvJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network definition\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Flatten 28x28 image to 784-element vector\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Network architecture\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # First hidden layer: 784 inputs -> 128 neurons\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),  # Activation function (introduce non-linearity)\n",
        "\n",
        "            # Second hidden layer: 128 inputs -> 64 neurons\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer: 64 inputs -> 10 outputs (digits 0-9)\n",
        "            nn.Linear(hidden_size2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)  # logits = raw prediction scores\n",
        "        return logits\n",
        "\n",
        "model = NeuralNet().to(device)  # Move model to GPU if available"
      ],
      "metadata": {
        "id": "ObnGUYOJC3HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Combines softmax + negative log likelihood\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adaptive learning rate"
      ],
      "metadata": {
        "id": "1rEbou4hDCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "        # Print progress\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Testing phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation (saves memory)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get class with highest probability\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mnist_ann.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWeptUrLDHMs",
        "outputId": "8c46c419-a5a8-4150-caa2-b308e15b0ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 0.5569\n",
            "Epoch [1/10], Step [200/938], Loss: 0.4424\n",
            "Epoch [1/10], Step [300/938], Loss: 0.2649\n",
            "Epoch [1/10], Step [400/938], Loss: 0.1822\n",
            "Epoch [1/10], Step [500/938], Loss: 0.1494\n",
            "Epoch [1/10], Step [600/938], Loss: 0.2107\n",
            "Epoch [1/10], Step [700/938], Loss: 0.1494\n",
            "Epoch [1/10], Step [800/938], Loss: 0.1463\n",
            "Epoch [1/10], Step [900/938], Loss: 0.2397\n",
            "Test Accuracy: 96.02%\n",
            "Epoch [2/10], Step [100/938], Loss: 0.0563\n",
            "Epoch [2/10], Step [200/938], Loss: 0.0933\n",
            "Epoch [2/10], Step [300/938], Loss: 0.1589\n",
            "Epoch [2/10], Step [400/938], Loss: 0.1921\n",
            "Epoch [2/10], Step [500/938], Loss: 0.0411\n",
            "Epoch [2/10], Step [600/938], Loss: 0.0876\n",
            "Epoch [2/10], Step [700/938], Loss: 0.1009\n",
            "Epoch [2/10], Step [800/938], Loss: 0.0553\n",
            "Epoch [2/10], Step [900/938], Loss: 0.1516\n",
            "Test Accuracy: 97.05%\n",
            "Epoch [3/10], Step [100/938], Loss: 0.0340\n",
            "Epoch [3/10], Step [200/938], Loss: 0.0446\n",
            "Epoch [3/10], Step [300/938], Loss: 0.0764\n",
            "Epoch [3/10], Step [400/938], Loss: 0.0223\n",
            "Epoch [3/10], Step [500/938], Loss: 0.0472\n",
            "Epoch [3/10], Step [600/938], Loss: 0.1238\n",
            "Epoch [3/10], Step [700/938], Loss: 0.0932\n",
            "Epoch [3/10], Step [800/938], Loss: 0.0744\n",
            "Epoch [3/10], Step [900/938], Loss: 0.0686\n",
            "Test Accuracy: 97.26%\n",
            "Epoch [4/10], Step [100/938], Loss: 0.0446\n",
            "Epoch [4/10], Step [200/938], Loss: 0.0430\n",
            "Epoch [4/10], Step [300/938], Loss: 0.1842\n",
            "Epoch [4/10], Step [400/938], Loss: 0.0476\n",
            "Epoch [4/10], Step [500/938], Loss: 0.0168\n",
            "Epoch [4/10], Step [600/938], Loss: 0.0687\n",
            "Epoch [4/10], Step [700/938], Loss: 0.0085\n",
            "Epoch [4/10], Step [800/938], Loss: 0.0216\n",
            "Epoch [4/10], Step [900/938], Loss: 0.0149\n",
            "Test Accuracy: 97.57%\n",
            "Epoch [5/10], Step [100/938], Loss: 0.0157\n",
            "Epoch [5/10], Step [200/938], Loss: 0.0217\n",
            "Epoch [5/10], Step [300/938], Loss: 0.0694\n",
            "Epoch [5/10], Step [400/938], Loss: 0.0140\n",
            "Epoch [5/10], Step [500/938], Loss: 0.0125\n",
            "Epoch [5/10], Step [600/938], Loss: 0.0364\n",
            "Epoch [5/10], Step [700/938], Loss: 0.0910\n",
            "Epoch [5/10], Step [800/938], Loss: 0.1559\n",
            "Epoch [5/10], Step [900/938], Loss: 0.0034\n",
            "Test Accuracy: 97.59%\n",
            "Epoch [6/10], Step [100/938], Loss: 0.0447\n",
            "Epoch [6/10], Step [200/938], Loss: 0.0061\n",
            "Epoch [6/10], Step [300/938], Loss: 0.0114\n",
            "Epoch [6/10], Step [400/938], Loss: 0.0376\n",
            "Epoch [6/10], Step [500/938], Loss: 0.0049\n",
            "Epoch [6/10], Step [600/938], Loss: 0.0049\n",
            "Epoch [6/10], Step [700/938], Loss: 0.1355\n",
            "Epoch [6/10], Step [800/938], Loss: 0.0905\n",
            "Epoch [6/10], Step [900/938], Loss: 0.0629\n",
            "Test Accuracy: 97.61%\n",
            "Epoch [7/10], Step [100/938], Loss: 0.0017\n",
            "Epoch [7/10], Step [200/938], Loss: 0.0022\n",
            "Epoch [7/10], Step [300/938], Loss: 0.0372\n",
            "Epoch [7/10], Step [400/938], Loss: 0.0219\n",
            "Epoch [7/10], Step [500/938], Loss: 0.0169\n",
            "Epoch [7/10], Step [600/938], Loss: 0.0447\n",
            "Epoch [7/10], Step [700/938], Loss: 0.0661\n",
            "Epoch [7/10], Step [800/938], Loss: 0.0077\n",
            "Epoch [7/10], Step [900/938], Loss: 0.0127\n",
            "Test Accuracy: 97.80%\n",
            "Epoch [8/10], Step [100/938], Loss: 0.0385\n",
            "Epoch [8/10], Step [200/938], Loss: 0.0167\n",
            "Epoch [8/10], Step [300/938], Loss: 0.0285\n",
            "Epoch [8/10], Step [400/938], Loss: 0.0458\n",
            "Epoch [8/10], Step [500/938], Loss: 0.0083\n",
            "Epoch [8/10], Step [600/938], Loss: 0.0555\n",
            "Epoch [8/10], Step [700/938], Loss: 0.0207\n",
            "Epoch [8/10], Step [800/938], Loss: 0.0602\n",
            "Epoch [8/10], Step [900/938], Loss: 0.0332\n",
            "Test Accuracy: 97.79%\n",
            "Epoch [9/10], Step [100/938], Loss: 0.0061\n",
            "Epoch [9/10], Step [200/938], Loss: 0.0128\n",
            "Epoch [9/10], Step [300/938], Loss: 0.0727\n",
            "Epoch [9/10], Step [400/938], Loss: 0.0092\n",
            "Epoch [9/10], Step [500/938], Loss: 0.0468\n",
            "Epoch [9/10], Step [600/938], Loss: 0.0041\n",
            "Epoch [9/10], Step [700/938], Loss: 0.0022\n",
            "Epoch [9/10], Step [800/938], Loss: 0.0071\n",
            "Epoch [9/10], Step [900/938], Loss: 0.0613\n",
            "Test Accuracy: 97.72%\n",
            "Epoch [10/10], Step [100/938], Loss: 0.0465\n",
            "Epoch [10/10], Step [200/938], Loss: 0.0186\n",
            "Epoch [10/10], Step [300/938], Loss: 0.0163\n",
            "Epoch [10/10], Step [400/938], Loss: 0.0044\n",
            "Epoch [10/10], Step [500/938], Loss: 0.0023\n",
            "Epoch [10/10], Step [600/938], Loss: 0.0015\n",
            "Epoch [10/10], Step [700/938], Loss: 0.0015\n",
            "Epoch [10/10], Step [800/938], Loss: 0.0009\n",
            "Epoch [10/10], Step [900/938], Loss: 0.0053\n",
            "Test Accuracy: 97.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Key Concepts Explained:**\n",
        "\n",
        "1. **MNIST Dataset**:\n",
        "   - 70,000 handwritten digits (60k train + 10k test)\n",
        "   - 28x28 grayscale images (0=black, 1=white)\n",
        "   - Why use MNIST? Simple, standardized benchmark for learning\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "   - `ToTensor()`: Converts images to PyTorch tensors\n",
        "   - `Normalize()`: Helps neural network learn faster (values between -1 and 1)\n",
        "\n",
        "3. **Neural Network Architecture**:\n",
        "   - **Input Layer**: 784 neurons (one per pixel)\n",
        "   - **Hidden Layers** (128 â†’ 64 neurons):\n",
        "     - Learn complex patterns through hierarchical processing\n",
        "     - ReLU activation adds non-linearity (enables complex function learning)\n",
        "   - **Output Layer**: 10 neurons (probability scores for digits 0-9)\n",
        "\n",
        "4. **Activation Function (ReLU)**:\n",
        "   - Rectified Linear Unit: `f(x) = max(0, x)`\n",
        "   - Why ReLU? Solves vanishing gradient problem, computationally efficient\n",
        "\n",
        "5. **Loss Function (CrossEntropyLoss)**:\n",
        "   - Measures difference between predicted probabilities and true labels\n",
        "   - Combines softmax (convert to probabilities) and negative log likelihood (measure error)\n",
        "\n",
        "6. **Optimizer (Adam)**:\n",
        "   - Adaptive learning rate algorithm\n",
        "   - Combines benefits of RMSProp and Momentum\n",
        "   - Why Adam? Requires less tuning than plain SGD\n",
        "\n",
        "7. **Training Process**:\n",
        "   - **Epoch**: Full pass through the training data\n",
        "   - **Batch**: Subset of data used for single weight update\n",
        "   - **Forward Pass**: Compute predictions\n",
        "   - **Backward Pass**: Calculate gradients (backpropagation)\n",
        "   - **Weight Update**: Adjust parameters using optimizer\n",
        "\n",
        "8. **Evaluation**:\n",
        "   - `model.eval()`: Disables dropout/batch norm (if used)\n",
        "   - `torch.no_grad()`: Saves memory by not tracking gradients\n",
        "   - Accuracy: Percentage of correct predictions\n",
        "\n",
        "**Why This Architecture?**\n",
        "- Simple but effective for basic image classification\n",
        "- Two hidden layers provide enough capacity to learn digits\n",
        "- ReLU helps prevent gradient vanishing in deeper layers\n",
        "- Adam optimizer gives good default performance\n",
        "\n",
        "**Tips for Improvement**:\n",
        "1. Add dropout layers for regularization\n",
        "2. Try different activation functions (Leaky ReLU, ELU)\n",
        "3. Add batch normalization layers\n",
        "4. Use learning rate scheduling\n",
        "5. Increase model depth/width\n",
        "6. Add data augmentation\n",
        "\n",
        "This implementation typically achieves **~97% accuracy** after 10 epochs. With more advanced techniques (like CNNs), you can reach >99% accuracy."
      ],
      "metadata": {
        "id": "hBotvOA2mHz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the saved model (if needed)\n",
        "# model = NeuralNet().to(device)\n",
        "# model.load_state_dict(torch.load('mnist_ann.pth'))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Select a random test sample\n",
        "sample_index = 1234  # You can change this index to test different images\n",
        "image, true_label = test_dataset[sample_index]\n",
        "image = image.to(device)\n",
        "\n",
        "# Add batch dimension (since model expects batches)\n",
        "image = image.unsqueeze(0)  # Shape: [1, 1, 28, 28]\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities).item()\n",
        "\n",
        "# Convert image back to CPU and numpy for plotting\n",
        "image = image.squeeze().cpu().numpy()\n",
        "\n",
        "# Denormalize the image\n",
        "# (Reverse the normalization we did during preprocessing)\n",
        "image = image * 0.3081 + 0.1307  # mean + (image * std)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f'True Label: {true_label} | Predicted: {predicted_class}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Print confidence scores\n",
        "print(\"\\nPrediction Confidence:\")\n",
        "for i in range(10):\n",
        "    print(f\"Digit {i}: {probabilities[0][i].item()*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "5wOfOAvemNbw",
        "outputId": "6da2f84e-6222-417a-ec46-b8f86235205a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction Confidence:\n",
            "Digit 0: 0.00%\n",
            "Digit 1: 0.00%\n",
            "Digit 2: 0.00%\n",
            "Digit 3: 0.00%\n",
            "Digit 4: 0.00%\n",
            "Digit 5: 0.00%\n",
            "Digit 6: 0.00%\n",
            "Digit 7: 0.00%\n",
            "Digit 8: 100.00%\n",
            "Digit 9: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XohW5AA5p19N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}